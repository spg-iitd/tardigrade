{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NU_kTYpsy7F",
        "outputId": "f489e18c-0551-404f-eb16-b735e5567190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: scapy in /home/himanshi/.local/lib/python3.10/site-packages (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNslAWINtRmg",
        "outputId": "d63c5a74-3072-4eea-aceb-072953b64a82"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/ymirsky/Kitsune-py.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQTDm6zntTbf",
        "outputId": "dd468b3f-60b9-4693-f80f-1eb524293c37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data  kitsune  Kitsune_eval.ipynb  LICENSE  models  README.md  results\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrkkTKaYtah-",
        "outputId": "cd6ff736-cac7-4817-d5f1-5957440c7787"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTnWLee7umAb",
        "outputId": "e7a4cd8f-625f-4568-d580-dffe1484c052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/himanshi/courses/sem8/cod891/tardigrade/kitsune\n"
          ]
        }
      ],
      "source": [
        "%cd kitsune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lz16lB9tfoa",
        "outputId": "2ef8a7a3-8f21-4ed0-e833-7f5cbee40874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Importing Scapy Library\n"
          ]
        }
      ],
      "source": [
        "from kit_model import KitModel\n",
        "from kit_model_keyed import KeyedKitModel\n",
        "from utils import *\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "821Y7Ch2wYPf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sklearn in /home/himanshi/.local/lib/python3.10/site-packages (0.0.post1)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn\n",
        "\n",
        "from textwrap import fill\n",
        "import datetime\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.dates as mdate\n",
        "import sklearn.metrics as metrics\n",
        "from itertools import product\n",
        "from tqdm import tqdm\n",
        "from matplotlib import cm\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib\n",
        "import socket\n",
        "import multiprocessing as mp\n",
        "matplotlib.use('Agg')\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "import torch\n",
        "# matplotlib.rcParams['timezone']=\"Pacific/Auckland\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Tl-SjWS-975z"
      },
      "outputs": [],
      "source": [
        "def squeeze_features(fv, precision):\n",
        "    \"\"\"rounds features to siginificant figures\n",
        "    Args:\n",
        "        fv (array): feature vector.\n",
        "        precision (int): number of precisions to use.\n",
        "    Returns:\n",
        "        array: rounded array of floats.\n",
        "    \"\"\"\n",
        "    fv_positive = np.where(np.isfinite(fv) & (\n",
        "        fv != 0), np.abs(fv), 10**(precision-1))\n",
        "    mags = 10 ** (precision - 1 - np.floor(np.log10(fv_positive)))\n",
        "    return np.round(fv * mags) / mags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_kitsune(path, model, threshold=None, ignore_index=-1, out_image=None, meta_file=None, record_scores=False, y_true=None, record_prediction=False, load_prediction=False, plot_with_time=False):\n",
        "    \"\"\"\n",
        "    evaluates trained kitsune model on some traffic.\n",
        "    Args:\n",
        "        path (string): path to traffic feature file.\n",
        "        model_path (string): path to trained kitsune model.\n",
        "        threshold (float): anomaly threshold value, if None it calculates the threshold value as 3 std away from mean. Defaults to None.\n",
        "        ignore_index (int): number of features to ignore at the start. Defaults to -1.\n",
        "        out_image (string): path to output anomaly score image. Defaults to None.\n",
        "        meta_file (string): path to metadata file, used to calculate evasion metrics. Defaults to None.\n",
        "        record_scores (boolean): whether to record anomaly scores in a seperate csv file. Defaults to False.\n",
        "    Returns:\n",
        "        if has_meta: return number of positive samples and positive samples that are not craft packets.\n",
        "        else: return number of positive samples\n",
        "    \"\"\"\n",
        "    # the pcap, pcapng, or tsv file to process.\n",
        "    print(\"evaluting\", path)\n",
        "    print(\"meta\", meta_file)\n",
        "    \n",
        "    t = threshold\n",
        "    roc_auc = 1\n",
        "    label_map = []\n",
        "\n",
        "    # with open(model_path, \"rb\") as m:\n",
        "    #     kitsune = pickle.load(m)\n",
        "    \n",
        "    test_data = open(path, \"r\")\n",
        "\n",
        "\n",
        "    if out_image == None:\n",
        "        out_image = \"kitsune_rmse.png\"\n",
        "\n",
        "    if meta_file is not None:\n",
        "        meta = open(meta_file, \"r\")\n",
        "        meta.readline()\n",
        "        meta_row = meta.readline()\n",
        "        has_meta = True\n",
        "        pos_craft = 0\n",
        "        pos_mal = 0\n",
        "        pos_ignore = 0\n",
        "    else:\n",
        "        has_meta = False\n",
        "        pos = 0\n",
        "\n",
        "    labels = []\n",
        "    times = []\n",
        "    colours = []\n",
        "    tbar = tqdm()\n",
        "    if load_prediction:\n",
        "        rmse_array = np.genfromtxt(\n",
        "            \"kitsune_score.csv\", delimiter=\",\")\n",
        "    else:\n",
        "        counter = 0\n",
        "        rmse_array = []\n",
        "\n",
        "        if not has_meta:\n",
        "            colours = None\n",
        "\n",
        "        while True:\n",
        "\n",
        "            if counter < ignore_index:\n",
        "                if meta_file is not None:\n",
        "                    meta_row = meta.readline()\n",
        "\n",
        "                counter += 1\n",
        "                continue\n",
        "\n",
        "            pkt = test_data.readline()\n",
        "            if pkt == \"\":\n",
        "                break\n",
        "            pkt = pkt.rstrip().split(\",\")\n",
        "            pkt = [[float(x) for x in pkt]]\n",
        "            pkt = torch.tensor(pkt).float()\n",
        "\n",
        "            rmse = model.score(pkt)[0].item()\n",
        "        \n",
        "\n",
        "            if rmse==-1:\n",
        "                break\n",
        "\n",
        "            if rmse == 0:\n",
        "                rmse_array.append(1e-2)\n",
        "            \n",
        "            else:\n",
        "                rmse_array.append(rmse)\n",
        "            \n",
        "            counter += 1\n",
        "            tbar.update(1)\n",
        "\n",
        "            # set colours\n",
        "            if has_meta:\n",
        "                comment = meta_row.rstrip().split(\",\")[-1]\n",
        "                if comment == \"craft\":\n",
        "                    colours.append([67 / 255., 67 / 255., 67 / 255., 0.8])\n",
        "\n",
        "                elif comment == \"malicious\":\n",
        "                    colours.append([1, 0, 0, 1])\n",
        "                else:\n",
        "                    colours.append([204 / 255., 243 / 255., 1, 0.5])\n",
        "\n",
        "            if threshold is not None and rmse > threshold:\n",
        "                if has_meta:\n",
        "                    comment = meta_row.rstrip().split(\",\")[-1]\n",
        "                    if comment == \"craft\":\n",
        "                        pos_craft += 1\n",
        "                    elif comment == \"malicious\":\n",
        "                        pos_mal += 1\n",
        "                    elif comment == \"attacker_low\":\n",
        "                        pos_ignore += 1\n",
        "                    else:\n",
        "                        print(meta_row)\n",
        "                        print(rmse)\n",
        "                        raise Exception\n",
        "                else:\n",
        "                    pos += 1\n",
        "\n",
        "            if has_meta:\n",
        "                meta_row = meta.readline()\n",
        "\n",
        "    # if no threshold, calculate threshold\n",
        "    if threshold == None:\n",
        "        # threshold is min(mean+3std, max)\n",
        "        benignSample = np.log(rmse_array)\n",
        "        mean = np.mean(benignSample)\n",
        "        std = np.std(benignSample)\n",
        "        threshold_std = np.exp(mean + 3 * std)\n",
        "        threshold_max = max(rmse_array)\n",
        "        threshold = min(threshold_max, threshold_std)\n",
        "        pos = (rmse_array > threshold).sum()\n",
        "\n",
        "    # record prediction scores/rmse\n",
        "    if record_scores:\n",
        "        score_path = \"kitsune_score.csv\"\n",
        "        threshold_path = \"kitsune_threshold.csv\"\n",
        "        # print(\"max_rmse\",np.max(rmse_array))\n",
        "        np.savetxt(score_path, rmse_array, delimiter=\",\")\n",
        "        np.savetxt(threshold_path, [threshold], delimiter=\",\")\n",
        "        print(\"score saved to\", score_path)\n",
        "\n",
        "    # record prediction labels\n",
        "    if record_prediction:\n",
        "        pred_path = \"kitsune_prediction.csv\"\n",
        "        # np.savetxt(pred_path, rmse_array > threshold, delimiter=\",\")\n",
        "        np.savetxt(pred_path, np.where(np.array(rmse_array) >= threshold)[0], delimiter=\",\")\n",
        "        print(\"kitsune prediction saved to\", pred_path)\n",
        "\n",
        "    if y_true is None:\n",
        "\n",
        "        fpr, tpr, roc_t = metrics.roc_curve(\n",
        "            [0 for i in range(len(rmse_array))], rmse_array, drop_intermediate=False)\n",
        "    else:\n",
        "        fpr, tpr, roc_t = metrics.roc_curve(\n",
        "            y_true, rmse_array, drop_intermediate=True)\n",
        "        roc_auc = metrics.auc(fpr, tpr)\n",
        "    print(\"total packets:\", len(rmse_array))\n",
        "\n",
        "    if out_image is not None:\n",
        "        cmap = plt.get_cmap('Set3')\n",
        "        num_packets = len(rmse_array)\n",
        "        f, (ax1, ax2) = plt.subplots(\n",
        "            2, 1, constrained_layout=True, figsize=(10, 10), dpi=200)\n",
        "\n",
        "        if times and plot_with_time:\n",
        "            x_val = times\n",
        "            date_fmt = '%m/%d %H:%M:%S'\n",
        "\n",
        "            date_formatter = mdate.DateFormatter(date_fmt)\n",
        "            ax1.xaxis.set_major_formatter(date_formatter)\n",
        "\n",
        "            # tick every 4 hours\n",
        "            # print(\"asdfs\")\n",
        "            ax1.xaxis.set_major_locator(ticker.MultipleLocator(1 / 6))\n",
        "\n",
        "            ax1.tick_params(labelrotation=90)\n",
        "            # f.autofmt_xdate()\n",
        "        else:\n",
        "            x_val = range(len(rmse_array))\n",
        "\n",
        "        if labels:\n",
        "            (unique, counts) = np.unique(labels, return_counts=True)\n",
        "            frequencies = np.asarray((unique, counts)).T\n",
        "            for i in frequencies:\n",
        "                label_map[i[0]] = \"{} {}\".format(label_map[i[0]], i[1])\n",
        "\n",
        "            scatter = ax1.scatter(x_val, rmse_array,\n",
        "                                  s=1, c=labels, alpha=0.05, cmap=cmap)\n",
        "            # wrap legends\n",
        "            labels = [fill(l, 20) for l in label_map]\n",
        "\n",
        "            leg = ax1.legend(handles=scatter.legend_elements()[0], labels=labels, bbox_to_anchor=(1.01, 1),\n",
        "                             loc='upper left', borderaxespad=0.)\n",
        "            for lh in leg.legendHandles:\n",
        "                lh._legmarker.set_alpha(1.)\n",
        "\n",
        "        elif has_meta:\n",
        "            ax1.scatter(x_val, rmse_array, s=1, c=colours)\n",
        "        else:\n",
        "            ax1.scatter(x_val, rmse_array, s=1, alpha=0.05)\n",
        "\n",
        "        # max_rmse=np.max(rmse_array)\n",
        "        # print(max_rmse)\n",
        "\n",
        "        ax1.axhline(y=threshold, color='r', linestyle='-')\n",
        "        ax1.set_yscale(\"log\")\n",
        "        # ax1.set_title(\"Anomaly Scores from Kitsune_{} Execution Phase\".format(\n",
        "        #     model_path.split(\"/\")[-1]))\n",
        "        ax1.set_ylabel(\"RMSE (log scaled)\")\n",
        "        if has_meta:\n",
        "            ax1.set_xlabel(\n",
        "                \"packet index \\n packets over threshold {}\".format(pos_mal + pos_craft))\n",
        "        else:\n",
        "            ax1.set_xlabel(\n",
        "                \"packet index \\n packets over threshold {}\".format(pos))\n",
        "\n",
        "        if y_true is None:\n",
        "            ax2.plot(fpr, roc_t, 'b')\n",
        "            ax2.set_ylabel(\"threshold\")\n",
        "            ax2.set_xlabel(\"false positive rate\")\n",
        "        else:\n",
        "            ax2.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
        "            ax2.set_title('AUC = %0.2f' % roc_auc)\n",
        "            ax2.set_ylabel(\"true positive rate\")\n",
        "            ax2.set_xlabel(\"false positive rate\")\n",
        "        # plt.tight_layout()\n",
        "        f.savefig(out_image)\n",
        "        print(\"plot path:\", out_image)\n",
        "        plt.close()\n",
        "    tbar.close()\n",
        "    if has_meta:\n",
        "        return pos_mal, pos_craft, pos_ignore\n",
        "    else:\n",
        "        if t is None:\n",
        "            return pos, threshold\n",
        "        else:\n",
        "            return pos, roc_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eak6pCxtW495",
        "outputId": "7c1ae022-b767-4ac8-82fc-9dffb62e4683"
      },
      "outputs": [],
      "source": [
        "# print(\"Unzipping Sample Capture...\")\n",
        "# import zipfile\n",
        "# with zipfile.ZipFile(\"mirai.zip\",\"r\") as zip_ref:\n",
        "#     zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGAoHEJg-R_q",
        "outputId": "49b8bcb7-0c6c-469d-d5e0-f7a3e20cfcb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model on ../Data/traffic.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/himanshi/.local/lib/python3.10/site-packages/torch/utils/data/datapipes/utils/common.py:137: UserWarning: Local function is not supported by pickle, please use regular python function or functools.partial instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch [0]  tail losses 5.68691  head loss: 0.12126\n",
            " Epoch [0]  tail losses 5.68010  head loss: 0.12159\n",
            " Epoch [0]  tail losses 5.67332  head loss: 0.12191\n",
            " Epoch [0]  tail losses 5.66631  head loss: 0.12224\n",
            " Epoch [0]  tail losses 5.65958  head loss: 0.12256\n",
            " Epoch [0]  tail losses 5.65230  head loss: 0.12292\n",
            " Epoch [0]  tail losses 5.64786  head loss: 0.12308\n",
            " Epoch [0]  tail losses 5.64287  head loss: 0.12329\n",
            " Epoch [0]  tail losses 5.63769  head loss: 0.12350\n",
            " Epoch [0]  tail losses 5.63232  head loss: 0.12373\n",
            " Epoch [0]  tail losses 5.62754  head loss: 0.12392\n",
            " Epoch [0]  tail losses 5.62164  head loss: 0.12418\n",
            " Epoch [0]  tail losses 5.61669  head loss: 0.12438\n",
            " Epoch [0]  tail losses 5.61203  head loss: 0.12456\n",
            " Epoch [0]  tail losses 5.60624  head loss: 0.12482\n",
            " Epoch [0]  tail losses 5.60110  head loss: 0.12503\n",
            " Epoch [0]  tail losses 5.59567  head loss: 0.12526\n",
            " Epoch [0]  tail losses 5.59090  head loss: 0.12545\n",
            " Epoch [0]  tail losses 5.58604  head loss: 0.12565\n",
            " Epoch [0]  tail losses 5.58131  head loss: 0.12583\n",
            " Epoch [0]  tail losses 5.57651  head loss: 0.12602\n",
            " Epoch [0]  tail losses 5.57201  head loss: 0.12619\n",
            " Epoch [0]  tail losses 5.56728  head loss: 0.12638\n",
            " Epoch [0]  tail losses 5.56272  head loss: 0.12655\n",
            " Epoch [0]  tail losses 5.55815  head loss: 0.12673\n",
            " Epoch [0]  tail losses 5.55363  head loss: 0.12690\n",
            " Epoch [0]  tail losses 5.54895  head loss: 0.12708\n",
            " Epoch [0]  tail losses 5.54379  head loss: 0.12730\n",
            " Epoch [0]  tail losses 5.53913  head loss: 0.12748\n",
            " Epoch [0]  tail losses 5.53480  head loss: 0.12764\n",
            " Epoch [0]  tail losses 5.53006  head loss: 0.12783\n",
            " Epoch [0]  tail losses 5.52546  head loss: 0.12801\n",
            " Epoch [0]  tail losses 5.52101  head loss: 0.12817\n",
            " Epoch [0]  tail losses 5.51654  head loss: 0.12834\n",
            " Epoch [0]  tail losses 5.51209  head loss: 0.12851\n",
            " Epoch [0]  tail losses 5.50746  head loss: 0.12869\n",
            " Epoch [0]  tail losses 5.50281  head loss: 0.12887\n",
            " Epoch [0]  tail losses 5.49819  head loss: 0.12905\n",
            " Epoch [0]  tail losses 5.49344  head loss: 0.12924\n",
            " Epoch [0]  tail losses 5.48879  head loss: 0.12942\n",
            " Epoch [0]  tail losses 5.48411  head loss: 0.12961\n",
            " Epoch [0]  tail losses 5.47961  head loss: 0.12978\n",
            " Epoch [0]  tail losses 5.47507  head loss: 0.12995\n",
            " Epoch [0]  tail losses 5.47051  head loss: 0.13013\n",
            " Epoch [0]  tail losses 5.46579  head loss: 0.13032\n",
            " Epoch [0]  tail losses 5.46114  head loss: 0.13050\n",
            " Epoch [0]  tail losses 5.45664  head loss: 0.13067\n",
            " Epoch [0]  tail losses 5.45196  head loss: 0.13086\n",
            " Epoch [0]  tail losses 5.44748  head loss: 0.13103\n",
            " Epoch [0]  tail losses 5.44300  head loss: 0.13120\n",
            " Epoch [0]  tail losses 5.43856  head loss: 0.13137\n",
            " Epoch [0]  tail losses 5.43382  head loss: 0.13155\n",
            " Epoch [0]  tail losses 5.42918  head loss: 0.13174\n",
            " Epoch [0]  tail losses 5.42468  head loss: 0.13191\n",
            " Epoch [0]  tail losses 5.42012  head loss: 0.13208\n",
            " Epoch [0]  tail losses 5.41561  head loss: 0.13226\n",
            " Epoch [0]  tail losses 5.41117  head loss: 0.13243\n",
            " Epoch [0]  tail losses 5.40662  head loss: 0.13260\n",
            " Epoch [0]  tail losses 5.40208  head loss: 0.13278\n",
            " Epoch [0]  tail losses 5.39742  head loss: 0.13296\n",
            " Epoch [0]  tail losses 5.39224  head loss: 0.13318\n",
            " Epoch [0]  tail losses 5.38672  head loss: 0.13342\n",
            " Epoch [0]  tail losses 5.38115  head loss: 0.13366\n",
            " Epoch [0]  tail losses 5.37535  head loss: 0.13392\n",
            " Epoch [0]  tail losses 5.36921  head loss: 0.13420\n",
            " Epoch [0]  tail losses 5.36285  head loss: 0.13450\n",
            " Epoch [0]  tail losses 5.35654  head loss: 0.13480\n",
            " Epoch [0]  tail losses 5.35018  head loss: 0.13509\n",
            " Epoch [0]  tail losses 5.34381  head loss: 0.13539\n",
            " Epoch [0]  tail losses 5.33749  head loss: 0.13568\n",
            " Epoch [0]  tail losses 5.33118  head loss: 0.13598\n",
            " Epoch [0]  tail losses 5.32440  head loss: 0.13630\n",
            " Epoch [0]  tail losses 5.31770  head loss: 0.13662\n",
            " Epoch [0]  tail losses 5.31087  head loss: 0.13695\n",
            " Epoch [0]  tail losses 5.30417  head loss: 0.13727\n",
            " Epoch [0]  tail losses 5.29747  head loss: 0.13759\n",
            " Epoch [0]  tail losses 5.29087  head loss: 0.13790\n",
            " Epoch [0]  tail losses 5.28435  head loss: 0.13821\n",
            " Epoch [0]  tail losses 5.27786  head loss: 0.13852\n",
            " Epoch [0]  tail losses 5.27153  head loss: 0.13881\n",
            " Epoch [0]  tail losses 5.26528  head loss: 0.13910\n",
            " Epoch [0]  tail losses 5.25925  head loss: 0.13938\n",
            " Epoch [0]  tail losses 5.25323  head loss: 0.13965\n",
            " Epoch [0]  tail losses 5.24736  head loss: 0.13992\n",
            " Epoch [0]  tail losses 5.24159  head loss: 0.14017\n",
            " Epoch [0]  tail losses 5.23583  head loss: 0.14043\n",
            " Epoch [0]  tail losses 5.23019  head loss: 0.14068\n",
            " Epoch [0]  tail losses 5.22444  head loss: 0.14094\n",
            " Epoch [0]  tail losses 5.21856  head loss: 0.14120\n",
            " Epoch [0]  tail losses 5.21261  head loss: 0.14147\n",
            " Epoch [0]  tail losses 5.20679  head loss: 0.14174\n",
            " Epoch [0]  tail losses 5.20093  head loss: 0.14200\n",
            " Epoch [0]  tail losses 5.19517  head loss: 0.14226\n",
            " Epoch [0]  tail losses 5.18956  head loss: 0.14251\n",
            " Epoch [0]  tail losses 5.18405  head loss: 0.14275\n",
            " Epoch [0]  tail losses 5.17851  head loss: 0.14299\n",
            " Epoch [0]  tail losses 5.17311  head loss: 0.14322\n",
            " Epoch [0]  tail losses 5.16780  head loss: 0.14345\n",
            " Epoch [0]  tail losses 5.16242  head loss: 0.14368\n",
            " Epoch [0]  tail losses 5.15722  head loss: 0.14390\n",
            " Epoch [0]  tail losses 5.15205  head loss: 0.14412\n",
            " Epoch [0]  tail losses 5.14700  head loss: 0.14433\n",
            " Epoch [0]  tail losses 5.14190  head loss: 0.14455\n",
            " Epoch [0]  tail losses 5.13688  head loss: 0.14476\n",
            " Epoch [0]  tail losses 5.13196  head loss: 0.14496\n",
            " Epoch [0]  tail losses 5.12709  head loss: 0.14516\n",
            " Epoch [0]  tail losses 5.12225  head loss: 0.14535\n",
            " Epoch [0]  tail losses 5.11740  head loss: 0.14555\n",
            " Epoch [0]  tail losses 5.11258  head loss: 0.14575\n",
            " Epoch [0]  tail losses 5.10788  head loss: 0.14594\n",
            " Epoch [0]  tail losses 5.10317  head loss: 0.14612\n",
            " Epoch [0]  tail losses 5.09848  head loss: 0.14631\n",
            " Epoch [0]  tail losses 5.09384  head loss: 0.14650\n",
            " Epoch [0]  tail losses 5.08920  head loss: 0.14668\n",
            " Epoch [0]  tail losses 5.08459  head loss: 0.14686\n",
            " Epoch [0]  tail losses 5.08004  head loss: 0.14704\n",
            " Epoch [0]  tail losses 5.07552  head loss: 0.14722\n",
            " Epoch [0]  tail losses 5.07097  head loss: 0.14739\n",
            " Epoch [0]  tail losses 5.06654  head loss: 0.14756\n",
            " Epoch [0]  tail losses 5.06207  head loss: 0.14774\n",
            " Epoch [0]  tail losses 5.05763  head loss: 0.14791\n",
            " Epoch [0]  tail losses 5.05318  head loss: 0.14808\n",
            " Epoch [0]  tail losses 5.04874  head loss: 0.14825\n",
            " Epoch [0]  tail losses 5.04428  head loss: 0.14842\n",
            " Epoch [0]  tail losses 5.03986  head loss: 0.14859\n",
            " Epoch [0]  tail losses 5.03551  head loss: 0.14876\n",
            " Epoch [0]  tail losses 5.03113  head loss: 0.14893\n",
            " Epoch [0]  tail losses 5.02676  head loss: 0.14909\n",
            " Epoch [0]  tail losses 5.02242  head loss: 0.14926\n",
            " Epoch [0]  tail losses 5.01812  head loss: 0.14942\n",
            " Epoch [0]  tail losses 5.01383  head loss: 0.14958\n",
            " Epoch [0]  tail losses 5.00954  head loss: 0.14974\n",
            " Epoch [0]  tail losses 5.00525  head loss: 0.14991\n",
            " Epoch [0]  tail losses 5.00104  head loss: 0.15006\n",
            " Epoch [0]  tail losses 4.99681  head loss: 0.15022\n",
            " Epoch [0]  tail losses 4.99261  head loss: 0.15038\n",
            " Epoch [0]  tail losses 4.98845  head loss: 0.15053\n",
            " Epoch [0]  tail losses 4.98425  head loss: 0.15069\n",
            " Epoch [0]  tail losses 4.98008  head loss: 0.15084\n",
            " Epoch [0]  tail losses 4.97588  head loss: 0.15100\n",
            " Epoch [0]  tail losses 4.97166  head loss: 0.15115\n",
            " Epoch [0]  tail losses 4.96750  head loss: 0.15131\n",
            " Epoch [0]  tail losses 4.96336  head loss: 0.15146\n",
            " Epoch [0]  tail losses 4.95924  head loss: 0.15161\n",
            " Epoch [0]  tail losses 4.95510  head loss: 0.15177\n",
            " Epoch [0]  tail losses 4.95100  head loss: 0.15192\n",
            " Epoch [0]  tail losses 4.94688  head loss: 0.15207\n",
            " Epoch [0]  tail losses 4.94276  head loss: 0.15222\n",
            " Epoch [0]  tail losses 4.93868  head loss: 0.15237\n",
            " Epoch [0]  tail losses 4.93455  head loss: 0.15252\n",
            " Epoch [0]  tail losses 4.93047  head loss: 0.15267\n",
            " Epoch [0]  tail losses 4.92638  head loss: 0.15282\n",
            " Epoch [0]  tail losses 4.92224  head loss: 0.15298\n",
            " Epoch [0]  tail losses 4.91810  head loss: 0.15313\n",
            " Epoch [0]  tail losses 4.91398  head loss: 0.15328\n",
            " Epoch [0]  tail losses 4.90989  head loss: 0.15343\n",
            " Epoch [0]  tail losses 4.90580  head loss: 0.15358\n",
            " Epoch [0]  tail losses 4.90169  head loss: 0.15373\n",
            " Epoch [0]  tail losses 4.89758  head loss: 0.15389\n",
            " Epoch [0]  tail losses 4.89345  head loss: 0.15404\n",
            " Epoch [0]  tail losses 4.88935  head loss: 0.15419\n",
            " Epoch [0]  tail losses 4.88529  head loss: 0.15434\n",
            " Epoch [0]  tail losses 4.88122  head loss: 0.15449\n",
            " Epoch [0]  tail losses 4.87714  head loss: 0.15464\n",
            " Epoch [0]  tail losses 4.87309  head loss: 0.15479\n",
            " Epoch [0]  tail losses 4.86903  head loss: 0.15494\n",
            " Epoch [0]  tail losses 4.86498  head loss: 0.15509\n",
            " Epoch [0]  tail losses 4.86094  head loss: 0.15523\n",
            " Epoch [0]  tail losses 4.85686  head loss: 0.15539\n",
            " Epoch [0]  tail losses 4.85282  head loss: 0.15554\n",
            " Epoch [0]  tail losses 4.84875  head loss: 0.15568\n",
            " Epoch [0]  tail losses 4.84472  head loss: 0.15583\n",
            " Epoch [0]  tail losses 4.84068  head loss: 0.15598\n",
            " Epoch [0]  tail losses 4.83665  head loss: 0.15613\n",
            " Epoch [0]  tail losses 4.83262  head loss: 0.15628\n",
            " Epoch [0]  tail losses 4.82859  head loss: 0.15643\n",
            " Epoch [0]  tail losses 4.82456  head loss: 0.15657\n",
            " Epoch [0]  tail losses 4.82049  head loss: 0.15672\n",
            " Epoch [0]  tail losses 4.81644  head loss: 0.15687\n",
            " Epoch [0]  tail losses 4.81241  head loss: 0.15702\n",
            " Epoch [0]  tail losses 4.80838  head loss: 0.15717\n",
            " Epoch [0]  tail losses 4.80440  head loss: 0.15732\n",
            " Epoch [0]  tail losses 4.80040  head loss: 0.15746\n",
            " Epoch [0]  tail losses 4.79643  head loss: 0.15761\n",
            " Epoch [0]  tail losses 4.79240  head loss: 0.15776\n",
            " Epoch [0]  tail losses 4.78842  head loss: 0.15790\n",
            " Epoch [0]  tail losses 4.78444  head loss: 0.15805\n",
            " Epoch [0]  tail losses 4.78050  head loss: 0.15819\n",
            " Epoch [0]  tail losses 4.77653  head loss: 0.15833\n",
            " Epoch [0]  tail losses 4.77256  head loss: 0.15848\n",
            " Epoch [0]  tail losses 4.76861  head loss: 0.15862\n",
            " Epoch [0]  tail losses 4.76469  head loss: 0.15877\n",
            " Epoch [0]  tail losses 4.76075  head loss: 0.15891\n",
            " Epoch [0]  tail losses 4.75678  head loss: 0.15905\n",
            " Epoch [0]  tail losses 4.75282  head loss: 0.15920\n",
            " Epoch [0]  tail losses 4.74891  head loss: 0.15934\n",
            " Epoch [0]  tail losses 4.74496  head loss: 0.15949\n",
            " Epoch [0]  tail losses 4.74103  head loss: 0.15963\n",
            " Epoch [0]  tail losses 4.73715  head loss: 0.15977\n",
            " Epoch [0]  tail losses 4.73326  head loss: 0.15991\n",
            " Epoch [0]  tail losses 4.72933  head loss: 0.16005\n",
            " Epoch [0]  tail losses 4.72539  head loss: 0.16020\n",
            " Epoch [0]  tail losses 4.72146  head loss: 0.16034\n",
            " Epoch [0]  tail losses 4.71760  head loss: 0.16048\n",
            " Epoch [0]  tail losses 4.71371  head loss: 0.16062\n",
            " Epoch [0]  tail losses 4.70983  head loss: 0.16076\n",
            " Epoch [0]  tail losses 4.70594  head loss: 0.16090\n",
            " Epoch [0]  tail losses 4.70210  head loss: 0.16104\n",
            " Epoch [0]  tail losses 4.69823  head loss: 0.16118\n",
            " Epoch [0]  tail losses 4.69438  head loss: 0.16131\n",
            " Epoch [0]  tail losses 4.69054  head loss: 0.16145\n",
            " Epoch [0]  tail losses 4.68667  head loss: 0.16159\n",
            " Epoch [0]  tail losses 4.68282  head loss: 0.16173\n",
            " Epoch [0]  tail losses 4.67898  head loss: 0.16187\n",
            " Epoch [0]  tail losses 4.67514  head loss: 0.16201\n",
            " Epoch [0]  tail losses 4.67129  head loss: 0.16214\n",
            " Epoch [0]  tail losses 4.66748  head loss: 0.16228\n",
            " Epoch [0]  tail losses 4.66367  head loss: 0.16242\n",
            " Epoch [0]  tail losses 4.65985  head loss: 0.16255\n",
            " Epoch [0]  tail losses 4.65601  head loss: 0.16269\n",
            " Epoch [0]  tail losses 4.65221  head loss: 0.16283\n",
            " Epoch [0]  tail losses 4.64842  head loss: 0.16296\n",
            " Epoch [0]  tail losses 4.64464  head loss: 0.16309\n",
            " Epoch [0]  tail losses 4.64084  head loss: 0.16323\n",
            " Epoch [0]  tail losses 4.63706  head loss: 0.16336\n",
            " Epoch [0]  tail losses 4.63325  head loss: 0.16350\n",
            " Epoch [0]  tail losses 4.62948  head loss: 0.16363\n",
            " Epoch [0]  tail losses 4.62570  head loss: 0.16377\n",
            " Epoch [0]  tail losses 4.62194  head loss: 0.16390\n",
            " Epoch [0]  tail losses 4.61821  head loss: 0.16403\n",
            " Epoch [0]  tail losses 4.61448  head loss: 0.16416\n",
            " Epoch [0]  tail losses 4.61070  head loss: 0.16430\n",
            " Epoch [0]  tail losses 4.60696  head loss: 0.16443\n",
            " Epoch [0]  tail losses 4.60326  head loss: 0.16456\n",
            " Epoch [0]  tail losses 4.59956  head loss: 0.16469\n",
            " Epoch [0]  tail losses 4.59588  head loss: 0.16482\n",
            " Epoch [0]  tail losses 4.59218  head loss: 0.16495\n",
            " Epoch [0]  tail losses 4.58847  head loss: 0.16508\n",
            " Epoch [0]  tail losses 4.58479  head loss: 0.16521\n",
            " Epoch [0]  tail losses 4.58110  head loss: 0.16533\n",
            " Epoch [0]  tail losses 4.57742  head loss: 0.16546\n",
            " Epoch [0]  tail losses 4.57375  head loss: 0.16559\n",
            " Epoch [0]  tail losses 4.57009  head loss: 0.16572\n",
            " Epoch [0]  tail losses 4.56641  head loss: 0.16585\n",
            " Epoch [0]  tail losses 4.56274  head loss: 0.16597\n",
            " Epoch [0]  tail losses 4.55907  head loss: 0.16610\n",
            " Epoch [0]  tail losses 4.55540  head loss: 0.16623\n",
            " Epoch [0]  tail losses 4.55172  head loss: 0.16636\n",
            " Epoch [0]  tail losses 4.54808  head loss: 0.16648\n",
            " Epoch [0]  tail losses 4.54443  head loss: 0.16661\n",
            " Epoch [0]  tail losses 4.54078  head loss: 0.16674\n",
            " Epoch [0]  tail losses 4.53715  head loss: 0.16686\n",
            " Epoch [0]  tail losses 4.53352  head loss: 0.16699\n",
            " Epoch [0]  tail losses 4.52986  head loss: 0.16711\n",
            " Epoch [0]  tail losses 4.52623  head loss: 0.16724\n",
            " Epoch [0]  tail losses 4.52261  head loss: 0.16736\n",
            " Epoch [0]  tail losses 4.51901  head loss: 0.16749\n",
            " Epoch [0]  tail losses 4.51541  head loss: 0.16761\n",
            " Epoch [0]  tail losses 4.51180  head loss: 0.16774\n",
            " Epoch [0]  tail losses 4.50818  head loss: 0.16786\n",
            " Epoch [0]  tail losses 4.50460  head loss: 0.16798\n",
            " Epoch [0]  tail losses 4.50101  head loss: 0.16811\n",
            " Epoch [0]  tail losses 4.49741  head loss: 0.16823\n",
            " Epoch [0]  tail losses 4.49382  head loss: 0.16835\n",
            " Epoch [0]  tail losses 4.49025  head loss: 0.16847\n",
            " Epoch [0]  tail losses 4.48668  head loss: 0.16860\n",
            " Epoch [0]  tail losses 4.48313  head loss: 0.16872\n",
            " Epoch [0]  tail losses 4.47957  head loss: 0.16884\n",
            " Epoch [0]  tail losses 4.47602  head loss: 0.16896\n",
            " Epoch [0]  tail losses 4.47248  head loss: 0.16908\n",
            " Epoch [0]  tail losses 4.46893  head loss: 0.16920\n",
            " Epoch [0]  tail losses 4.46542  head loss: 0.16932\n",
            " Epoch [0]  tail losses 4.46189  head loss: 0.16944\n",
            " Epoch [0]  tail losses 4.45835  head loss: 0.16955\n",
            " Epoch [0]  tail losses 4.45482  head loss: 0.16967\n",
            " Epoch [0]  tail losses 4.45130  head loss: 0.16979\n",
            " Epoch [0]  tail losses 4.44779  head loss: 0.16991\n",
            " Epoch [0]  tail losses 4.44428  head loss: 0.17003\n",
            " Epoch [0]  tail losses 4.44075  head loss: 0.17015\n",
            " Epoch [0]  tail losses 4.43724  head loss: 0.17027\n",
            " Epoch [0]  tail losses 4.43377  head loss: 0.17038\n",
            " Epoch [0]  tail losses 4.43031  head loss: 0.17050\n",
            " Epoch [0]  tail losses 4.42683  head loss: 0.17061\n",
            " Epoch [0]  tail losses 4.42334  head loss: 0.17073\n",
            " Epoch [0]  tail losses 4.41987  head loss: 0.17085\n",
            " Epoch [0]  tail losses 4.41640  head loss: 0.17096\n",
            " Epoch [0]  tail losses 4.41296  head loss: 0.17107\n",
            " Epoch [0]  tail losses 4.40950  head loss: 0.17119\n",
            " Epoch [0]  tail losses 4.40606  head loss: 0.17130\n",
            " Epoch [0]  tail losses 4.40261  head loss: 0.17142\n",
            " Epoch [0]  tail losses 4.39919  head loss: 0.17153\n",
            " Epoch [0]  tail losses 4.39576  head loss: 0.17164\n",
            " Epoch [0]  tail losses 4.39232  head loss: 0.17176\n",
            " Epoch [0]  tail losses 4.38891  head loss: 0.17187\n",
            " Epoch [0]  tail losses 4.38550  head loss: 0.17198\n",
            " Epoch [0]  tail losses 4.38209  head loss: 0.17209\n",
            " Epoch [0]  tail losses 4.37864  head loss: 0.17221\n",
            " Epoch [0]  tail losses 4.37521  head loss: 0.17232\n",
            " Epoch [0]  tail losses 4.37179  head loss: 0.17243\n",
            " Epoch [0]  tail losses 4.36837  head loss: 0.17255\n",
            " Epoch [0]  tail losses 4.36495  head loss: 0.17266\n",
            " Epoch [0]  tail losses 4.36153  head loss: 0.17277\n",
            " Epoch [0]  tail losses 4.35814  head loss: 0.17288\n",
            " Epoch [0]  tail losses 4.35476  head loss: 0.17299\n",
            " Epoch [0]  tail losses 4.35138  head loss: 0.17310\n",
            " Epoch [0]  tail losses 4.34801  head loss: 0.17321\n",
            " Epoch [0]  tail losses 4.34462  head loss: 0.17332\n",
            " Epoch [0]  tail losses 4.34125  head loss: 0.17343\n",
            " Epoch [0]  tail losses 4.33788  head loss: 0.17354\n",
            " Epoch [0]  tail losses 4.33452  head loss: 0.17365\n",
            " Epoch [0]  tail losses 4.33114  head loss: 0.17376\n",
            " Epoch [0]  tail losses 4.32779  head loss: 0.17386\n",
            " Epoch [0]  tail losses 4.32442  head loss: 0.17397\n",
            " Epoch [0]  tail losses 4.32106  head loss: 0.17408\n",
            " Epoch [0]  tail losses 4.31773  head loss: 0.17419\n",
            " Epoch [0]  tail losses 4.31441  head loss: 0.17430\n",
            " Epoch [0]  tail losses 4.31108  head loss: 0.17440\n",
            " Epoch [0]  tail losses 4.30777  head loss: 0.17451\n",
            " Epoch [0]  tail losses 4.30449  head loss: 0.17461\n",
            " Epoch [0]  tail losses 4.30117  head loss: 0.17472\n",
            " Epoch [0]  tail losses 4.29784  head loss: 0.17483\n",
            " Epoch [0]  tail losses 4.29452  head loss: 0.17493\n",
            " Epoch [0]  tail losses 4.29125  head loss: 0.17504\n",
            " Epoch [0]  tail losses 4.28791  head loss: 0.17514\n",
            " Epoch [0]  tail losses 4.28463  head loss: 0.17525\n",
            " Epoch [0]  tail losses 4.28133  head loss: 0.17535\n",
            " Epoch [0]  tail losses 4.27804  head loss: 0.17546\n",
            " Epoch [0]  tail losses 4.27476  head loss: 0.17556\n",
            " Epoch [0]  tail losses 4.27148  head loss: 0.17566\n",
            " Epoch [0]  tail losses 4.26820  head loss: 0.17577\n",
            " Epoch [0]  tail losses 4.26492  head loss: 0.17587\n",
            " Epoch [0]  tail losses 4.26162  head loss: 0.17598\n",
            " Epoch [0]  tail losses 4.25836  head loss: 0.17608\n",
            " Epoch [0]  tail losses 4.25509  head loss: 0.17618\n",
            " Epoch [0]  tail losses 4.25182  head loss: 0.17629\n",
            " Epoch [0]  tail losses 4.24859  head loss: 0.17639\n",
            " Epoch [0]  tail losses 4.24534  head loss: 0.17649\n",
            " Epoch [0]  tail losses 4.24208  head loss: 0.17659\n",
            " Epoch [0]  tail losses 4.23887  head loss: 0.17669\n",
            " Epoch [0]  tail losses 4.23563  head loss: 0.17679\n",
            " Epoch [0]  tail losses 4.23244  head loss: 0.17689\n",
            " Epoch [0]  tail losses 4.22921  head loss: 0.17699\n",
            " Epoch [0]  tail losses 4.22598  head loss: 0.17709\n",
            " Epoch [0]  tail losses 4.22278  head loss: 0.17719\n",
            " Epoch [0]  tail losses 4.21957  head loss: 0.17729\n",
            " Epoch [0]  tail losses 4.21638  head loss: 0.17739\n",
            " Epoch [0]  tail losses 4.21316  head loss: 0.17749\n",
            " Epoch [0]  tail losses 4.20995  head loss: 0.17759\n",
            " Epoch [0]  tail losses 4.20674  head loss: 0.17769\n",
            " Epoch [0]  tail losses 4.20354  head loss: 0.17779\n",
            " Epoch [0]  tail losses 4.20037  head loss: 0.17788\n",
            " Epoch [0]  tail losses 4.19718  head loss: 0.17798\n",
            " Epoch [0]  tail losses 4.19397  head loss: 0.17808\n",
            " Epoch [0]  tail losses 4.19078  head loss: 0.17818\n",
            " Epoch [0]  tail losses 4.18760  head loss: 0.17828\n",
            " Epoch [0]  tail losses 4.18439  head loss: 0.17838\n",
            " Epoch [0]  tail losses 4.18122  head loss: 0.17847\n",
            " Epoch [0]  tail losses 4.17806  head loss: 0.17857\n",
            " Epoch [0]  tail losses 4.17492  head loss: 0.17867\n",
            " Epoch [0]  tail losses 4.17180  head loss: 0.17876\n",
            " Epoch [0]  tail losses 4.16864  head loss: 0.17886\n",
            " Epoch [0]  tail losses 4.16549  head loss: 0.17895\n",
            " Epoch [0]  tail losses 4.16236  head loss: 0.17905\n",
            " Epoch [0]  tail losses 4.15924  head loss: 0.17914\n",
            " Epoch [0]  tail losses 4.15609  head loss: 0.17924\n",
            " Epoch [0]  tail losses 4.15298  head loss: 0.17933\n",
            " Epoch [0]  tail losses 4.14987  head loss: 0.17942\n",
            " Epoch [0]  tail losses 4.14674  head loss: 0.17952\n",
            " Epoch [0]  tail losses 4.14363  head loss: 0.17961\n",
            " Epoch [0]  tail losses 4.14056  head loss: 0.17970\n",
            " Epoch [0]  tail losses 4.13745  head loss: 0.17980\n",
            " Epoch [0]  tail losses 4.13438  head loss: 0.17989\n",
            " Epoch [0]  tail losses 4.13128  head loss: 0.17998\n",
            " Epoch [0]  tail losses 4.12818  head loss: 0.18007\n",
            " Epoch [0]  tail losses 4.12509  head loss: 0.18016\n",
            " Epoch [0]  tail losses 4.12204  head loss: 0.18025\n",
            " Epoch [0]  tail losses 4.11897  head loss: 0.18034\n",
            " Epoch [0]  tail losses 4.11589  head loss: 0.18044\n",
            " Epoch [0]  tail losses 4.11283  head loss: 0.18053\n",
            " Epoch [0]  tail losses 4.10978  head loss: 0.18062\n",
            " Epoch [0]  tail losses 4.10671  head loss: 0.18071\n",
            " Epoch [0]  tail losses 4.10364  head loss: 0.18080\n",
            " Epoch [0]  tail losses 4.10061  head loss: 0.18089\n",
            " Epoch [0]  tail losses 4.09752  head loss: 0.18098\n",
            " Epoch [0]  tail losses 4.09446  head loss: 0.18107\n",
            " Epoch [0]  tail losses 4.09140  head loss: 0.18116\n",
            " Epoch [0]  tail losses 4.08837  head loss: 0.18124\n",
            " Epoch [0]  tail losses 4.08535  head loss: 0.18133\n",
            " Epoch [0]  tail losses 4.08233  head loss: 0.18142\n",
            " Epoch [0]  tail losses 4.07930  head loss: 0.18151\n",
            " Epoch [0]  tail losses 4.07626  head loss: 0.18160\n",
            " Epoch [0]  tail losses 4.07326  head loss: 0.18168\n",
            " Epoch [0]  tail losses 4.07021  head loss: 0.18177\n",
            " Epoch [0]  tail losses 4.06719  head loss: 0.18186\n",
            " Epoch [0]  tail losses 4.06418  head loss: 0.18195\n",
            " Epoch [0]  tail losses 4.06118  head loss: 0.18203\n",
            " Epoch [0]  tail losses 4.05822  head loss: 0.18212\n",
            " Epoch [0]  tail losses 4.05524  head loss: 0.18220\n",
            "Training complete\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n    Keyed Kitsune Evaluation\\n'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "    Kitsune Evaluation\n",
        "\"\"\"\n",
        "\n",
        "data = \"../Data/traffic.csv\"\n",
        "\n",
        "# Extract features\n",
        "# print(\"Extracting features\")\n",
        "# extract_features(\"../Data/traffic.tsv\", \"../Data/traffic.csv\")\n",
        "\n",
        "\n",
        "# Train model on \"../Data/traffic.csv\"\n",
        "print(\"Training model on \" + data)\n",
        "model = KitModel()\n",
        "\n",
        "# Train model\n",
        "model.train_model(data)\n",
        "print(\"Training complete\")\n",
        "\n",
        "# Store model\n",
        "model.store_model(\"kitsune.pt\")\n",
        "\n",
        "\"\"\"\n",
        "    Keyed Kitsune Evaluation\n",
        "\"\"\"\n",
        "\n",
        "# data = \"../Data/_transformed_traffic.csv\"\n",
        "\n",
        "# # # Transform features\n",
        "# # print(\"Transforming features\")\n",
        "# # transform_features(\"../Data/traffic.csv\", \"../Data/transformed_traffic.csv\")\n",
        "\n",
        "# # Train model on \"../Data/traffic.csv\"\n",
        "# print(\"Training Keyed Kitsune model on \" + data)\n",
        "# model = KeyedKitModel()\n",
        "\n",
        "# # Train model\n",
        "# model.train_model(data)\n",
        "# print(\"Training complete\")\n",
        "\n",
        "# # Store model\n",
        "# model.store_model(\"KeyedKitsune.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gweftj1V-NXE",
        "outputId": "a7c70b40-55a3-4116-d262-bbce8b51d8c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluting ../Data/traffic.csv\n",
            "meta None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "764124it [1:05:28, 276.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score saved to kitsune_score.csv\n",
            "kitsune prediction saved to kitsune_prediction.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/himanshi/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1029: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total packets: 764136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "764136it [1:05:36, 194.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plot path: KeyedKitsuneRes_png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(100, 0.24941907297291574)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# eval_kitsune(path,K,record_prediction=True)\n",
        "eval_kitsune(data, model, threshold=None, out_image=\"KeyedKitsuneRes_png\", meta_file=None, record_scores=True, y_true=None, record_prediction=True, load_prediction=False, plot_with_time=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qwDhrlu_Gbj",
        "outputId": "b620d6f5-de01-4554-cbaf-99b1814ddae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'adversarial-kitsune'...\n",
            "Password for 'https://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@github.com': "
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/swainsubrat/adversarial-kitsune.git\n",
        "!git clone https://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@github.com/swainsubrat/adversarial-kitsune.git\n",
        "\n",
        "# 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def evaluation_metrics(scores_path, threshold_path):\n",
        "    # Calulate the F1 score for Model using ground labels\n",
        "    # Actual outputs from kitsune_score.csv\n",
        "    y_output = csv.reader(open(scores_path, 'r'))\n",
        "    y_output = list(y_output)\n",
        "    y_output = [float(i[0]) for i in y_output]\n",
        "\n",
        "    # let ground labels be 0(benign) for all pkts\n",
        "    y_true = np.ones(len(y_output))\n",
        "\n",
        "    # Threshold for F1 score\n",
        "    threshold = csv.reader(open(threshold_path, 'r'))\n",
        "    threshold = list(threshold)\n",
        "    threshold = float(threshold[0][0])\n",
        "\n",
        "    # Predicted labels\n",
        "    y_pred = np.array([0 if i > threshold else 1 for i in y_output])\n",
        "\n",
        "    # 0 -> benign and 1 -> malicious\n",
        "\n",
        "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    f1 = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "    print(\"True Negatives: \", tn)\n",
        "    print(\"False Positives: \", fp)\n",
        "    print(\"False Negatives: \", fn)\n",
        "    print(\"True Positives: \", tp)\n",
        "\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 score: \", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UO97zWTVt3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation metrics for Kitsune: \n",
            "True Negatives:  0\n",
            "False Positives:  0\n",
            "False Negatives:  64\n",
            "True Positives:  764072\n",
            "Precision:  1.0\n",
            "Recall:  0.9999162452757101\n",
            "F1 score:  0.9999581208840681\n",
            "\n",
            "Evaluation metrics for Keyed Kitsune: \n",
            "True Negatives:  0\n",
            "False Positives:  0\n",
            "False Negatives:  139\n",
            "True Positives:  763997\n",
            "Precision:  1.0\n",
            "Recall:  0.9998180952081829\n",
            "F1 score:  0.9999090393310006\n"
          ]
        }
      ],
      "source": [
        "# Evaluation metrics for Kitsune\n",
        "scores_path = '../results/Kitsune/kitsune_score.csv'\n",
        "threshold_path = '../results/Kitsune/kitsune_threshold.csv'\n",
        "\n",
        "print(\"Evaluation metrics for Kitsune: \")\n",
        "evaluation_metrics(scores_path, threshold_path)\n",
        "\n",
        "print()\n",
        "\n",
        "# Evaluation metrics for Keyed Kitsune\n",
        "scores_path = '../results/KeyedKitsune/kitsune_score.csv'\n",
        "threshold_path = '../results/KeyedKitsune/kitsune_threshold.csv'\n",
        "\n",
        "print(\"Evaluation metrics for Keyed Kitsune: \")\n",
        "evaluation_metrics(scores_path, threshold_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
