{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NU_kTYpsy7F",
        "outputId": "f489e18c-0551-404f-eb16-b735e5567190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: scapy in /home/himanshi/.local/lib/python3.10/site-packages (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNslAWINtRmg",
        "outputId": "d63c5a74-3072-4eea-aceb-072953b64a82"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/ymirsky/Kitsune-py.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQTDm6zntTbf",
        "outputId": "dd468b3f-60b9-4693-f80f-1eb524293c37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AfterImage.py\t  FeatureExtractor.py  __main__.py  pyproject.toml\n",
            "anomaly_score.py  __init__.py\t       models.py    train.py\n",
            "data.py\t\t  kit_model_keyed.py   netStat.py   Transformation.py\n",
            "engine.py\t  kit_model.py\t       __pycache__  utils.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrkkTKaYtah-",
        "outputId": "cd6ff736-cac7-4817-d5f1-5957440c7787"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTnWLee7umAb",
        "outputId": "e7a4cd8f-625f-4568-d580-dffe1484c052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'kitsune'\n",
            "/home/himanshi/courses/sem8/cod891/tardigrade/kitsune\n"
          ]
        }
      ],
      "source": [
        "%cd kitsune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lz16lB9tfoa",
        "outputId": "2ef8a7a3-8f21-4ed0-e833-7f5cbee40874"
      },
      "outputs": [],
      "source": [
        "from kit_model import KitModel\n",
        "from kit_model_keyed import KeyedKitModel\n",
        "from utils import *\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "821Y7Ch2wYPf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sklearn in /home/himanshi/.local/lib/python3.10/site-packages (0.0.post1)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn\n",
        "\n",
        "from textwrap import fill\n",
        "import datetime\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.dates as mdate\n",
        "import sklearn.metrics as metrics\n",
        "from itertools import product\n",
        "from tqdm import tqdm\n",
        "from matplotlib import cm\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib\n",
        "import socket\n",
        "import multiprocessing as mp\n",
        "matplotlib.use('Agg')\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "import torch\n",
        "# matplotlib.rcParams['timezone']=\"Pacific/Auckland\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Tl-SjWS-975z"
      },
      "outputs": [],
      "source": [
        "def squeeze_features(fv, precision):\n",
        "    \"\"\"rounds features to siginificant figures\n",
        "    Args:\n",
        "        fv (array): feature vector.\n",
        "        precision (int): number of precisions to use.\n",
        "    Returns:\n",
        "        array: rounded array of floats.\n",
        "    \"\"\"\n",
        "    fv_positive = np.where(np.isfinite(fv) & (\n",
        "        fv != 0), np.abs(fv), 10**(precision-1))\n",
        "    mags = 10 ** (precision - 1 - np.floor(np.log10(fv_positive)))\n",
        "    return np.round(fv * mags) / mags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_kitsune(path, model, threshold=None, ignore_index=-1, out_image=None, meta_file=None, record_scores=False, y_true=None, record_prediction=False, load_prediction=False, plot_with_time=False):\n",
        "    \"\"\"\n",
        "    evaluates trained kitsune model on some traffic.\n",
        "    Args:\n",
        "        path (string): path to traffic feature file.\n",
        "        model_path (string): path to trained kitsune model.\n",
        "        threshold (float): anomaly threshold value, if None it calculates the threshold value as 3 std away from mean. Defaults to None.\n",
        "        ignore_index (int): number of features to ignore at the start. Defaults to -1.\n",
        "        out_image (string): path to output anomaly score image. Defaults to None.\n",
        "        meta_file (string): path to metadata file, used to calculate evasion metrics. Defaults to None.\n",
        "        record_scores (boolean): whether to record anomaly scores in a seperate csv file. Defaults to False.\n",
        "    Returns:\n",
        "        if has_meta: return number of positive samples and positive samples that are not craft packets.\n",
        "        else: return number of positive samples\n",
        "    \"\"\"\n",
        "    # the pcap, pcapng, or tsv file to process.\n",
        "    print(\"evaluting\", path)\n",
        "    print(\"meta\", meta_file)\n",
        "    \n",
        "    t = threshold\n",
        "    roc_auc = 1\n",
        "    label_map = []\n",
        "\n",
        "    # with open(model_path, \"rb\") as m:\n",
        "    #     kitsune = pickle.load(m)\n",
        "    \n",
        "    test_data = open(path, \"r\")\n",
        "\n",
        "\n",
        "    if out_image == None:\n",
        "        out_image = \"kitsune_rmse.png\"\n",
        "\n",
        "    if meta_file is not None:\n",
        "        meta = open(meta_file, \"r\")\n",
        "        meta.readline()\n",
        "        meta_row = meta.readline()\n",
        "        has_meta = True\n",
        "        pos_craft = 0\n",
        "        pos_mal = 0\n",
        "        pos_ignore = 0\n",
        "    else:\n",
        "        has_meta = False\n",
        "        pos = 0\n",
        "\n",
        "    labels = []\n",
        "    times = []\n",
        "    colours = []\n",
        "    tbar = tqdm()\n",
        "    if load_prediction:\n",
        "        rmse_array = np.genfromtxt(\n",
        "            \"kitsune_score.csv\", delimiter=\",\")\n",
        "    else:\n",
        "        counter = 0\n",
        "        rmse_array = []\n",
        "\n",
        "        if not has_meta:\n",
        "            colours = None\n",
        "\n",
        "        while True:\n",
        "\n",
        "            if counter < ignore_index:\n",
        "                if meta_file is not None:\n",
        "                    meta_row = meta.readline()\n",
        "\n",
        "                counter += 1\n",
        "                continue\n",
        "\n",
        "            pkt = test_data.readline()\n",
        "            if pkt == \"\":\n",
        "                break\n",
        "            pkt = pkt.rstrip().split(\",\")\n",
        "            pkt = [[float(x) for x in pkt]]\n",
        "            pkt = torch.tensor(pkt).float()\n",
        "\n",
        "            rmse = model.score(pkt)[0].item()\n",
        "        \n",
        "\n",
        "            if rmse==-1:\n",
        "                break\n",
        "\n",
        "            if rmse == 0:\n",
        "                rmse_array.append(1e-2)\n",
        "            \n",
        "            else:\n",
        "                rmse_array.append(rmse)\n",
        "            \n",
        "            counter += 1\n",
        "            tbar.update(1)\n",
        "\n",
        "            # set colours\n",
        "            if has_meta:\n",
        "                comment = meta_row.rstrip().split(\",\")[-1]\n",
        "                if comment == \"craft\":\n",
        "                    colours.append([67 / 255., 67 / 255., 67 / 255., 0.8])\n",
        "\n",
        "                elif comment == \"malicious\":\n",
        "                    colours.append([1, 0, 0, 1])\n",
        "                else:\n",
        "                    colours.append([204 / 255., 243 / 255., 1, 0.5])\n",
        "\n",
        "            if threshold is not None and rmse > threshold:\n",
        "                if has_meta:\n",
        "                    comment = meta_row.rstrip().split(\",\")[-1]\n",
        "                    if comment == \"craft\":\n",
        "                        pos_craft += 1\n",
        "                    elif comment == \"malicious\":\n",
        "                        pos_mal += 1\n",
        "                    elif comment == \"attacker_low\":\n",
        "                        pos_ignore += 1\n",
        "                    else:\n",
        "                        print(meta_row)\n",
        "                        print(rmse)\n",
        "                        raise Exception\n",
        "                else:\n",
        "                    pos += 1\n",
        "\n",
        "            if has_meta:\n",
        "                meta_row = meta.readline()\n",
        "\n",
        "    # if no threshold, calculate threshold\n",
        "    if threshold == None:\n",
        "        # threshold is min(mean+3std, max)\n",
        "        benignSample = np.log(rmse_array)\n",
        "        mean = np.mean(benignSample)\n",
        "        std = np.std(benignSample)\n",
        "        threshold_std = np.exp(mean + 3 * std)\n",
        "        threshold_max = max(rmse_array)\n",
        "        threshold = min(threshold_max, threshold_std)\n",
        "        pos = (rmse_array > threshold).sum()\n",
        "\n",
        "    # record prediction scores/rmse\n",
        "    if record_scores:\n",
        "        score_path = \"kitsune_score.csv\"\n",
        "        threshold_path = \"kitsune_threshold.csv\"\n",
        "        # print(\"max_rmse\",np.max(rmse_array))\n",
        "        np.savetxt(score_path, rmse_array, delimiter=\",\")\n",
        "        np.savetxt(threshold_path, [threshold], delimiter=\",\")\n",
        "        print(\"score saved to\", score_path)\n",
        "\n",
        "    # record prediction labels\n",
        "    if record_prediction:\n",
        "        pred_path = \"kitsune_prediction.csv\"\n",
        "        # np.savetxt(pred_path, rmse_array > threshold, delimiter=\",\")\n",
        "        np.savetxt(pred_path, np.where(np.array(rmse_array) >= threshold)[0], delimiter=\",\")\n",
        "        print(\"kitsune prediction saved to\", pred_path)\n",
        "\n",
        "    if y_true is None:\n",
        "\n",
        "        fpr, tpr, roc_t = metrics.roc_curve(\n",
        "            [0 for i in range(len(rmse_array))], rmse_array, drop_intermediate=False)\n",
        "    else:\n",
        "        fpr, tpr, roc_t = metrics.roc_curve(\n",
        "            y_true, rmse_array, drop_intermediate=True)\n",
        "        roc_auc = metrics.auc(fpr, tpr)\n",
        "    print(\"total packets:\", len(rmse_array))\n",
        "\n",
        "    if out_image is not None:\n",
        "        cmap = plt.get_cmap('Set3')\n",
        "        num_packets = len(rmse_array)\n",
        "        f, (ax1, ax2) = plt.subplots(\n",
        "            2, 1, constrained_layout=True, figsize=(10, 10), dpi=200)\n",
        "\n",
        "        if times and plot_with_time:\n",
        "            x_val = times\n",
        "            date_fmt = '%m/%d %H:%M:%S'\n",
        "\n",
        "            date_formatter = mdate.DateFormatter(date_fmt)\n",
        "            ax1.xaxis.set_major_formatter(date_formatter)\n",
        "\n",
        "            # tick every 4 hours\n",
        "            # print(\"asdfs\")\n",
        "            ax1.xaxis.set_major_locator(ticker.MultipleLocator(1 / 6))\n",
        "\n",
        "            ax1.tick_params(labelrotation=90)\n",
        "            # f.autofmt_xdate()\n",
        "        else:\n",
        "            x_val = range(len(rmse_array))\n",
        "\n",
        "        if labels:\n",
        "            (unique, counts) = np.unique(labels, return_counts=True)\n",
        "            frequencies = np.asarray((unique, counts)).T\n",
        "            for i in frequencies:\n",
        "                label_map[i[0]] = \"{} {}\".format(label_map[i[0]], i[1])\n",
        "\n",
        "            scatter = ax1.scatter(x_val, rmse_array,\n",
        "                                  s=1, c=labels, alpha=0.05, cmap=cmap)\n",
        "            # wrap legends\n",
        "            labels = [fill(l, 20) for l in label_map]\n",
        "\n",
        "            leg = ax1.legend(handles=scatter.legend_elements()[0], labels=labels, bbox_to_anchor=(1.01, 1),\n",
        "                             loc='upper left', borderaxespad=0.)\n",
        "            for lh in leg.legendHandles:\n",
        "                lh._legmarker.set_alpha(1.)\n",
        "\n",
        "        elif has_meta:\n",
        "            ax1.scatter(x_val, rmse_array, s=1, c=colours)\n",
        "        else:\n",
        "            ax1.scatter(x_val, rmse_array, s=1, alpha=0.05)\n",
        "\n",
        "        # max_rmse=np.max(rmse_array)\n",
        "        # print(max_rmse)\n",
        "\n",
        "        ax1.axhline(y=threshold, color='r', linestyle='-')\n",
        "        ax1.set_yscale(\"log\")\n",
        "        # ax1.set_title(\"Anomaly Scores from Kitsune_{} Execution Phase\".format(\n",
        "        #     model_path.split(\"/\")[-1]))\n",
        "        ax1.set_ylabel(\"RMSE (log scaled)\")\n",
        "        if has_meta:\n",
        "            ax1.set_xlabel(\n",
        "                \"packet index \\n packets over threshold {}\".format(pos_mal + pos_craft))\n",
        "        else:\n",
        "            ax1.set_xlabel(\n",
        "                \"packet index \\n packets over threshold {}\".format(pos))\n",
        "\n",
        "        if y_true is None:\n",
        "            ax2.plot(fpr, roc_t, 'b')\n",
        "            ax2.set_ylabel(\"threshold\")\n",
        "            ax2.set_xlabel(\"false positive rate\")\n",
        "        else:\n",
        "            ax2.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
        "            ax2.set_title('AUC = %0.2f' % roc_auc)\n",
        "            ax2.set_ylabel(\"true positive rate\")\n",
        "            ax2.set_xlabel(\"false positive rate\")\n",
        "        # plt.tight_layout()\n",
        "        f.savefig(out_image)\n",
        "        print(\"plot path:\", out_image)\n",
        "        plt.close()\n",
        "    tbar.close()\n",
        "    if has_meta:\n",
        "        return pos_mal, pos_craft, pos_ignore\n",
        "    else:\n",
        "        if t is None:\n",
        "            return pos, threshold\n",
        "        else:\n",
        "            return pos, roc_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eak6pCxtW495",
        "outputId": "7c1ae022-b767-4ac8-82fc-9dffb62e4683"
      },
      "outputs": [],
      "source": [
        "# print(\"Unzipping Sample Capture...\")\n",
        "# import zipfile\n",
        "# with zipfile.ZipFile(\"mirai.zip\",\"r\") as zip_ref:\n",
        "#     zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGAoHEJg-R_q",
        "outputId": "49b8bcb7-0c6c-469d-d5e0-f7a3e20cfcb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Keyed Kitsune model on ../Data/_transformed_traffic.csv\n",
            " Epoch [0]  tail losses 5.67221  head loss: 0.12198\n",
            " Epoch [0]  tail losses 5.67260  head loss: 0.12183\n",
            " Epoch [0]  tail losses 5.66655  head loss: 0.12210\n",
            " Epoch [0]  tail losses 5.66257  head loss: 0.12224\n",
            " Epoch [0]  tail losses 5.65677  head loss: 0.12250\n",
            " Epoch [0]  tail losses 5.65179  head loss: 0.12270\n",
            " Epoch [0]  tail losses 5.64632  head loss: 0.12294\n",
            " Epoch [0]  tail losses 5.64067  head loss: 0.12319\n",
            " Epoch [0]  tail losses 5.63566  head loss: 0.12339\n",
            " Epoch [0]  tail losses 5.63071  head loss: 0.12360\n",
            " Epoch [0]  tail losses 5.62569  head loss: 0.12380\n",
            " Epoch [0]  tail losses 5.62021  head loss: 0.12404\n",
            " Epoch [0]  tail losses 5.61503  head loss: 0.12426\n",
            " Epoch [0]  tail losses 5.60924  head loss: 0.12452\n",
            " Epoch [0]  tail losses 5.60311  head loss: 0.12480\n",
            " Epoch [0]  tail losses 5.59768  head loss: 0.12503\n",
            " Epoch [0]  tail losses 5.59261  head loss: 0.12524\n",
            " Epoch [0]  tail losses 5.58769  head loss: 0.12544\n",
            " Epoch [0]  tail losses 5.58288  head loss: 0.12564\n",
            " Epoch [0]  tail losses 5.57845  head loss: 0.12580\n",
            " Epoch [0]  tail losses 5.57362  head loss: 0.12600\n",
            " Epoch [0]  tail losses 5.56877  head loss: 0.12620\n",
            " Epoch [0]  tail losses 5.56445  head loss: 0.12636\n",
            " Epoch [0]  tail losses 5.55977  head loss: 0.12654\n",
            " Epoch [0]  tail losses 5.55537  head loss: 0.12671\n",
            " Epoch [0]  tail losses 5.55070  head loss: 0.12689\n",
            " Epoch [0]  tail losses 5.54603  head loss: 0.12708\n",
            " Epoch [0]  tail losses 5.54128  head loss: 0.12727\n",
            " Epoch [0]  tail losses 5.53663  head loss: 0.12745\n",
            " Epoch [0]  tail losses 5.53216  head loss: 0.12762\n",
            " Epoch [0]  tail losses 5.52754  head loss: 0.12780\n",
            " Epoch [0]  tail losses 5.52293  head loss: 0.12798\n",
            " Epoch [0]  tail losses 5.51848  head loss: 0.12815\n",
            " Epoch [0]  tail losses 5.51402  head loss: 0.12832\n",
            " Epoch [0]  tail losses 5.50958  head loss: 0.12849\n",
            " Epoch [0]  tail losses 5.50513  head loss: 0.12866\n",
            " Epoch [0]  tail losses 5.50043  head loss: 0.12885\n",
            " Epoch [0]  tail losses 5.49599  head loss: 0.12902\n",
            " Epoch [0]  tail losses 5.49117  head loss: 0.12921\n",
            " Epoch [0]  tail losses 5.48667  head loss: 0.12938\n",
            " Epoch [0]  tail losses 5.48200  head loss: 0.12957\n",
            " Epoch [0]  tail losses 5.47745  head loss: 0.12975\n",
            " Epoch [0]  tail losses 5.47278  head loss: 0.12993\n",
            " Epoch [0]  tail losses 5.46813  head loss: 0.13011\n",
            " Epoch [0]  tail losses 5.46361  head loss: 0.13029\n",
            " Epoch [0]  tail losses 5.45906  head loss: 0.13047\n",
            " Epoch [0]  tail losses 5.45455  head loss: 0.13064\n",
            " Epoch [0]  tail losses 5.45001  head loss: 0.13082\n",
            " Epoch [0]  tail losses 5.44552  head loss: 0.13099\n",
            " Epoch [0]  tail losses 5.44111  head loss: 0.13116\n",
            " Epoch [0]  tail losses 5.43655  head loss: 0.13133\n",
            " Epoch [0]  tail losses 5.43204  head loss: 0.13151\n",
            " Epoch [0]  tail losses 5.42746  head loss: 0.13169\n",
            " Epoch [0]  tail losses 5.42287  head loss: 0.13187\n",
            " Epoch [0]  tail losses 5.41833  head loss: 0.13204\n",
            " Epoch [0]  tail losses 5.41401  head loss: 0.13220\n",
            " Epoch [0]  tail losses 5.40947  head loss: 0.13238\n",
            " Epoch [0]  tail losses 5.40501  head loss: 0.13255\n",
            " Epoch [0]  tail losses 5.40058  head loss: 0.13272\n",
            " Epoch [0]  tail losses 5.39581  head loss: 0.13291\n",
            " Epoch [0]  tail losses 5.39068  head loss: 0.13313\n",
            " Epoch [0]  tail losses 5.38521  head loss: 0.13337\n",
            " Epoch [0]  tail losses 5.37956  head loss: 0.13362\n",
            " Epoch [0]  tail losses 5.37380  head loss: 0.13387\n",
            " Epoch [0]  tail losses 5.36773  head loss: 0.13415\n",
            " Epoch [0]  tail losses 5.36155  head loss: 0.13444\n",
            " Epoch [0]  tail losses 5.35527  head loss: 0.13473\n",
            " Epoch [0]  tail losses 5.34894  head loss: 0.13503\n",
            " Epoch [0]  tail losses 5.34258  head loss: 0.13533\n",
            " Epoch [0]  tail losses 5.33629  head loss: 0.13562\n",
            " Epoch [0]  tail losses 5.32988  head loss: 0.13592\n",
            " Epoch [0]  tail losses 5.32331  head loss: 0.13623\n",
            " Epoch [0]  tail losses 5.31655  head loss: 0.13656\n",
            " Epoch [0]  tail losses 5.30979  head loss: 0.13688\n",
            " Epoch [0]  tail losses 5.30302  head loss: 0.13721\n",
            " Epoch [0]  tail losses 5.29634  head loss: 0.13753\n",
            " Epoch [0]  tail losses 5.28981  head loss: 0.13783\n",
            " Epoch [0]  tail losses 5.28333  head loss: 0.13814\n",
            " Epoch [0]  tail losses 5.27700  head loss: 0.13844\n",
            " Epoch [0]  tail losses 5.27071  head loss: 0.13873\n",
            " Epoch [0]  tail losses 5.26460  head loss: 0.13901\n",
            " Epoch [0]  tail losses 5.25852  head loss: 0.13929\n",
            " Epoch [0]  tail losses 5.25255  head loss: 0.13956\n",
            " Epoch [0]  tail losses 5.24676  head loss: 0.13982\n",
            " Epoch [0]  tail losses 5.24096  head loss: 0.14008\n",
            " Epoch [0]  tail losses 5.23528  head loss: 0.14034\n",
            " Epoch [0]  tail losses 5.22964  head loss: 0.14059\n",
            " Epoch [0]  tail losses 5.22399  head loss: 0.14084\n",
            " Epoch [0]  tail losses 5.21819  head loss: 0.14110\n",
            " Epoch [0]  tail losses 5.21228  head loss: 0.14137\n",
            " Epoch [0]  tail losses 5.20643  head loss: 0.14163\n",
            " Epoch [0]  tail losses 5.20070  head loss: 0.14189\n",
            " Epoch [0]  tail losses 5.19507  head loss: 0.14214\n",
            " Epoch [0]  tail losses 5.18950  head loss: 0.14238\n",
            " Epoch [0]  tail losses 5.18404  head loss: 0.14262\n",
            " Epoch [0]  tail losses 5.17868  head loss: 0.14285\n",
            " Epoch [0]  tail losses 5.17330  head loss: 0.14309\n",
            " Epoch [0]  tail losses 5.16805  head loss: 0.14331\n",
            " Epoch [0]  tail losses 5.16271  head loss: 0.14354\n",
            " Epoch [0]  tail losses 5.15755  head loss: 0.14376\n",
            " Epoch [0]  tail losses 5.15242  head loss: 0.14398\n",
            " Epoch [0]  tail losses 5.14745  head loss: 0.14418\n",
            " Epoch [0]  tail losses 5.14239  head loss: 0.14439\n",
            " Epoch [0]  tail losses 5.13752  head loss: 0.14459\n",
            " Epoch [0]  tail losses 5.13260  head loss: 0.14480\n",
            " Epoch [0]  tail losses 5.12778  head loss: 0.14499\n",
            " Epoch [0]  tail losses 5.12297  head loss: 0.14519\n",
            " Epoch [0]  tail losses 5.11814  head loss: 0.14538\n",
            " Epoch [0]  tail losses 5.11342  head loss: 0.14557\n",
            " Epoch [0]  tail losses 5.10868  head loss: 0.14576\n",
            " Epoch [0]  tail losses 5.10403  head loss: 0.14595\n",
            " Epoch [0]  tail losses 5.09937  head loss: 0.14613\n",
            " Epoch [0]  tail losses 5.09480  head loss: 0.14631\n",
            " Epoch [0]  tail losses 5.09021  head loss: 0.14649\n",
            " Epoch [0]  tail losses 5.08562  head loss: 0.14668\n",
            " Epoch [0]  tail losses 5.08107  head loss: 0.14685\n",
            " Epoch [0]  tail losses 5.07657  head loss: 0.14703\n",
            " Epoch [0]  tail losses 5.07206  head loss: 0.14721\n",
            " Epoch [0]  tail losses 5.06763  head loss: 0.14738\n",
            " Epoch [0]  tail losses 5.06315  head loss: 0.14755\n",
            " Epoch [0]  tail losses 5.05875  head loss: 0.14772\n",
            " Epoch [0]  tail losses 5.05432  head loss: 0.14789\n",
            " Epoch [0]  tail losses 5.04991  head loss: 0.14806\n",
            " Epoch [0]  tail losses 5.04548  head loss: 0.14823\n",
            " Epoch [0]  tail losses 5.04106  head loss: 0.14840\n",
            " Epoch [0]  tail losses 5.03668  head loss: 0.14857\n",
            " Epoch [0]  tail losses 5.03234  head loss: 0.14873\n",
            " Epoch [0]  tail losses 5.02800  head loss: 0.14890\n",
            " Epoch [0]  tail losses 5.02370  head loss: 0.14906\n",
            " Epoch [0]  tail losses 5.01943  head loss: 0.14922\n",
            " Epoch [0]  tail losses 5.01515  head loss: 0.14938\n",
            " Epoch [0]  tail losses 5.01086  head loss: 0.14955\n",
            " Epoch [0]  tail losses 5.00662  head loss: 0.14970\n",
            " Epoch [0]  tail losses 5.00241  head loss: 0.14986\n",
            " Epoch [0]  tail losses 4.99819  head loss: 0.15002\n",
            " Epoch [0]  tail losses 4.99402  head loss: 0.15017\n",
            " Epoch [0]  tail losses 4.98986  head loss: 0.15033\n",
            " Epoch [0]  tail losses 4.98563  head loss: 0.15049\n",
            " Epoch [0]  tail losses 4.98144  head loss: 0.15064\n",
            " Epoch [0]  tail losses 4.97722  head loss: 0.15080\n",
            " Epoch [0]  tail losses 4.97298  head loss: 0.15096\n",
            " Epoch [0]  tail losses 4.96884  head loss: 0.15111\n",
            " Epoch [0]  tail losses 4.96468  head loss: 0.15127\n",
            " Epoch [0]  tail losses 4.96055  head loss: 0.15142\n",
            " Epoch [0]  tail losses 4.95638  head loss: 0.15157\n",
            " Epoch [0]  tail losses 4.95220  head loss: 0.15173\n",
            " Epoch [0]  tail losses 4.94808  head loss: 0.15188\n",
            " Epoch [0]  tail losses 4.94402  head loss: 0.15203\n",
            " Epoch [0]  tail losses 4.93988  head loss: 0.15218\n",
            " Epoch [0]  tail losses 4.93579  head loss: 0.15233\n",
            " Epoch [0]  tail losses 4.93166  head loss: 0.15249\n",
            " Epoch [0]  tail losses 4.92757  head loss: 0.15264\n",
            " Epoch [0]  tail losses 4.92345  head loss: 0.15279\n",
            " Epoch [0]  tail losses 4.91930  head loss: 0.15294\n",
            " Epoch [0]  tail losses 4.91517  head loss: 0.15310\n",
            " Epoch [0]  tail losses 4.91103  head loss: 0.15325\n",
            " Epoch [0]  tail losses 4.90693  head loss: 0.15340\n",
            " Epoch [0]  tail losses 4.90284  head loss: 0.15355\n",
            " Epoch [0]  tail losses 4.89875  head loss: 0.15371\n",
            " Epoch [0]  tail losses 4.89466  head loss: 0.15386\n",
            " Epoch [0]  tail losses 4.89060  head loss: 0.15401\n",
            " Epoch [0]  tail losses 4.88651  head loss: 0.15416\n",
            " Epoch [0]  tail losses 4.88243  head loss: 0.15431\n",
            " Epoch [0]  tail losses 4.87838  head loss: 0.15446\n",
            " Epoch [0]  tail losses 4.87431  head loss: 0.15461\n",
            " Epoch [0]  tail losses 4.87026  head loss: 0.15476\n",
            " Epoch [0]  tail losses 4.86622  head loss: 0.15490\n",
            " Epoch [0]  tail losses 4.86220  head loss: 0.15505\n",
            " Epoch [0]  tail losses 4.85818  head loss: 0.15520\n",
            " Epoch [0]  tail losses 4.85417  head loss: 0.15534\n",
            " Epoch [0]  tail losses 4.85013  head loss: 0.15549\n",
            " Epoch [0]  tail losses 4.84615  head loss: 0.15564\n",
            " Epoch [0]  tail losses 4.84215  head loss: 0.15578\n",
            " Epoch [0]  tail losses 4.83813  head loss: 0.15593\n",
            " Epoch [0]  tail losses 4.83415  head loss: 0.15608\n",
            " Epoch [0]  tail losses 4.83016  head loss: 0.15622\n",
            " Epoch [0]  tail losses 4.82618  head loss: 0.15637\n",
            " Epoch [0]  tail losses 4.82219  head loss: 0.15651\n",
            " Epoch [0]  tail losses 4.81823  head loss: 0.15666\n",
            " Epoch [0]  tail losses 4.81430  head loss: 0.15680\n",
            " Epoch [0]  tail losses 4.81034  head loss: 0.15694\n",
            " Epoch [0]  tail losses 4.80642  head loss: 0.15708\n",
            " Epoch [0]  tail losses 4.80247  head loss: 0.15723\n",
            " Epoch [0]  tail losses 4.79853  head loss: 0.15737\n",
            " Epoch [0]  tail losses 4.79458  head loss: 0.15751\n",
            " Epoch [0]  tail losses 4.79068  head loss: 0.15765\n",
            " Epoch [0]  tail losses 4.78676  head loss: 0.15779\n",
            " Epoch [0]  tail losses 4.78286  head loss: 0.15793\n",
            " Epoch [0]  tail losses 4.77903  head loss: 0.15807\n",
            " Epoch [0]  tail losses 4.77513  head loss: 0.15821\n",
            " Epoch [0]  tail losses 4.77123  head loss: 0.15835\n",
            " Epoch [0]  tail losses 4.76734  head loss: 0.15849\n",
            " Epoch [0]  tail losses 4.76343  head loss: 0.15863\n",
            " Epoch [0]  tail losses 4.75954  head loss: 0.15877\n",
            " Epoch [0]  tail losses 4.75566  head loss: 0.15891\n",
            " Epoch [0]  tail losses 4.75182  head loss: 0.15905\n",
            " Epoch [0]  tail losses 4.74796  head loss: 0.15919\n",
            " Epoch [0]  tail losses 4.74412  head loss: 0.15933\n",
            " Epoch [0]  tail losses 4.74028  head loss: 0.15946\n",
            " Epoch [0]  tail losses 4.73642  head loss: 0.15960\n",
            " Epoch [0]  tail losses 4.73257  head loss: 0.15974\n",
            " Epoch [0]  tail losses 4.72875  head loss: 0.15987\n",
            " Epoch [0]  tail losses 4.72498  head loss: 0.16001\n",
            " Epoch [0]  tail losses 4.72116  head loss: 0.16014\n",
            " Epoch [0]  tail losses 4.71734  head loss: 0.16028\n",
            " Epoch [0]  tail losses 4.71351  head loss: 0.16042\n",
            " Epoch [0]  tail losses 4.70970  head loss: 0.16055\n",
            " Epoch [0]  tail losses 4.70593  head loss: 0.16069\n",
            " Epoch [0]  tail losses 4.70214  head loss: 0.16082\n",
            " Epoch [0]  tail losses 4.69833  head loss: 0.16096\n",
            " Epoch [0]  tail losses 4.69456  head loss: 0.16109\n",
            " Epoch [0]  tail losses 4.69077  head loss: 0.16122\n",
            " Epoch [0]  tail losses 4.68698  head loss: 0.16136\n",
            " Epoch [0]  tail losses 4.68320  head loss: 0.16149\n",
            " Epoch [0]  tail losses 4.67943  head loss: 0.16163\n",
            " Epoch [0]  tail losses 4.67565  head loss: 0.16176\n",
            " Epoch [0]  tail losses 4.67187  head loss: 0.16189\n",
            " Epoch [0]  tail losses 4.66813  head loss: 0.16202\n",
            " Epoch [0]  tail losses 4.66440  head loss: 0.16215\n",
            " Epoch [0]  tail losses 4.66062  head loss: 0.16229\n",
            " Epoch [0]  tail losses 4.65689  head loss: 0.16242\n",
            " Epoch [0]  tail losses 4.65315  head loss: 0.16255\n",
            " Epoch [0]  tail losses 4.64941  head loss: 0.16268\n",
            " Epoch [0]  tail losses 4.64570  head loss: 0.16281\n",
            " Epoch [0]  tail losses 4.64200  head loss: 0.16294\n",
            " Epoch [0]  tail losses 4.63834  head loss: 0.16307\n",
            " Epoch [0]  tail losses 4.63462  head loss: 0.16320\n",
            " Epoch [0]  tail losses 4.63089  head loss: 0.16333\n",
            " Epoch [0]  tail losses 4.62722  head loss: 0.16346\n",
            " Epoch [0]  tail losses 4.62352  head loss: 0.16359\n",
            " Epoch [0]  tail losses 4.61986  head loss: 0.16371\n",
            " Epoch [0]  tail losses 4.61617  head loss: 0.16384\n",
            " Epoch [0]  tail losses 4.61249  head loss: 0.16397\n",
            " Epoch [0]  tail losses 4.60884  head loss: 0.16410\n",
            " Epoch [0]  tail losses 4.60517  head loss: 0.16422\n",
            " Epoch [0]  tail losses 4.60150  head loss: 0.16435\n",
            " Epoch [0]  tail losses 4.59787  head loss: 0.16448\n",
            " Epoch [0]  tail losses 4.59424  head loss: 0.16460\n",
            " Epoch [0]  tail losses 4.59060  head loss: 0.16473\n",
            " Epoch [0]  tail losses 4.58698  head loss: 0.16485\n",
            " Epoch [0]  tail losses 4.58337  head loss: 0.16497\n",
            " Epoch [0]  tail losses 4.57973  head loss: 0.16510\n",
            " Epoch [0]  tail losses 4.57614  head loss: 0.16522\n",
            " Epoch [0]  tail losses 4.57253  head loss: 0.16535\n",
            " Epoch [0]  tail losses 4.56891  head loss: 0.16547\n",
            " Epoch [0]  tail losses 4.56531  head loss: 0.16559\n",
            " Epoch [0]  tail losses 4.56172  head loss: 0.16572\n",
            " Epoch [0]  tail losses 4.55811  head loss: 0.16584\n",
            " Epoch [0]  tail losses 4.55453  head loss: 0.16596\n",
            " Epoch [0]  tail losses 4.55094  head loss: 0.16609\n",
            " Epoch [0]  tail losses 4.54736  head loss: 0.16621\n",
            " Epoch [0]  tail losses 4.54379  head loss: 0.16633\n",
            " Epoch [0]  tail losses 4.54022  head loss: 0.16645\n",
            " Epoch [0]  tail losses 4.53663  head loss: 0.16657\n",
            " Epoch [0]  tail losses 4.53303  head loss: 0.16670\n",
            " Epoch [0]  tail losses 4.52945  head loss: 0.16682\n",
            " Epoch [0]  tail losses 4.52589  head loss: 0.16694\n",
            " Epoch [0]  tail losses 4.52232  head loss: 0.16706\n",
            " Epoch [0]  tail losses 4.51878  head loss: 0.16718\n",
            " Epoch [0]  tail losses 4.51524  head loss: 0.16730\n",
            " Epoch [0]  tail losses 4.51168  head loss: 0.16742\n",
            " Epoch [0]  tail losses 4.50817  head loss: 0.16754\n",
            " Epoch [0]  tail losses 4.50466  head loss: 0.16766\n",
            " Epoch [0]  tail losses 4.50113  head loss: 0.16778\n",
            " Epoch [0]  tail losses 4.49762  head loss: 0.16789\n",
            " Epoch [0]  tail losses 4.49411  head loss: 0.16801\n",
            " Epoch [0]  tail losses 4.49058  head loss: 0.16813\n",
            " Epoch [0]  tail losses 4.48708  head loss: 0.16825\n",
            " Epoch [0]  tail losses 4.48358  head loss: 0.16836\n",
            " Epoch [0]  tail losses 4.48007  head loss: 0.16848\n",
            " Epoch [0]  tail losses 4.47657  head loss: 0.16860\n",
            " Epoch [0]  tail losses 4.47311  head loss: 0.16871\n",
            " Epoch [0]  tail losses 4.46962  head loss: 0.16883\n",
            " Epoch [0]  tail losses 4.46616  head loss: 0.16895\n",
            " Epoch [0]  tail losses 4.46269  head loss: 0.16906\n",
            " Epoch [0]  tail losses 4.45921  head loss: 0.16918\n",
            " Epoch [0]  tail losses 4.45574  head loss: 0.16929\n",
            " Epoch [0]  tail losses 4.45227  head loss: 0.16941\n",
            " Epoch [0]  tail losses 4.44882  head loss: 0.16952\n",
            " Epoch [0]  tail losses 4.44539  head loss: 0.16964\n",
            " Epoch [0]  tail losses 4.44196  head loss: 0.16975\n",
            " Epoch [0]  tail losses 4.43853  head loss: 0.16986\n",
            " Epoch [0]  tail losses 4.43510  head loss: 0.16997\n",
            " Epoch [0]  tail losses 4.43165  head loss: 0.17009\n",
            " Epoch [0]  tail losses 4.42822  head loss: 0.17020\n",
            " Epoch [0]  tail losses 4.42480  head loss: 0.17031\n",
            " Epoch [0]  tail losses 4.42140  head loss: 0.17042\n",
            " Epoch [0]  tail losses 4.41802  head loss: 0.17053\n",
            " Epoch [0]  tail losses 4.41461  head loss: 0.17064\n",
            " Epoch [0]  tail losses 4.41123  head loss: 0.17076\n",
            " Epoch [0]  tail losses 4.40783  head loss: 0.17087\n",
            " Epoch [0]  tail losses 4.40445  head loss: 0.17098\n",
            " Epoch [0]  tail losses 4.40107  head loss: 0.17109\n",
            " Epoch [0]  tail losses 4.39769  head loss: 0.17120\n",
            " Epoch [0]  tail losses 4.39433  head loss: 0.17130\n",
            " Epoch [0]  tail losses 4.39095  head loss: 0.17141\n",
            " Epoch [0]  tail losses 4.38758  head loss: 0.17152\n",
            " Epoch [0]  tail losses 4.38421  head loss: 0.17163\n",
            " Epoch [0]  tail losses 4.38085  head loss: 0.17174\n",
            " Epoch [0]  tail losses 4.37748  head loss: 0.17185\n",
            " Epoch [0]  tail losses 4.37410  head loss: 0.17196\n",
            " Epoch [0]  tail losses 4.37075  head loss: 0.17207\n",
            " Epoch [0]  tail losses 4.36742  head loss: 0.17218\n",
            " Epoch [0]  tail losses 4.36406  head loss: 0.17228\n",
            " Epoch [0]  tail losses 4.36071  head loss: 0.17239\n",
            " Epoch [0]  tail losses 4.35740  head loss: 0.17250\n",
            " Epoch [0]  tail losses 4.35406  head loss: 0.17261\n",
            " Epoch [0]  tail losses 4.35074  head loss: 0.17271\n",
            " Epoch [0]  tail losses 4.34742  head loss: 0.17282\n",
            " Epoch [0]  tail losses 4.34410  head loss: 0.17292\n",
            " Epoch [0]  tail losses 4.34077  head loss: 0.17303\n",
            " Epoch [0]  tail losses 4.33746  head loss: 0.17314\n",
            " Epoch [0]  tail losses 4.33414  head loss: 0.17324\n",
            " Epoch [0]  tail losses 4.33083  head loss: 0.17335\n",
            " Epoch [0]  tail losses 4.32750  head loss: 0.17345\n",
            " Epoch [0]  tail losses 4.32418  head loss: 0.17356\n",
            " Epoch [0]  tail losses 4.32089  head loss: 0.17366\n",
            " Epoch [0]  tail losses 4.31759  head loss: 0.17377\n",
            " Epoch [0]  tail losses 4.31431  head loss: 0.17387\n",
            " Epoch [0]  tail losses 4.31104  head loss: 0.17398\n",
            " Epoch [0]  tail losses 4.30774  head loss: 0.17408\n",
            " Epoch [0]  tail losses 4.30447  head loss: 0.17418\n",
            " Epoch [0]  tail losses 4.30123  head loss: 0.17429\n",
            " Epoch [0]  tail losses 4.29795  head loss: 0.17439\n",
            " Epoch [0]  tail losses 4.29466  head loss: 0.17449\n",
            " Epoch [0]  tail losses 4.29143  head loss: 0.17459\n",
            " Epoch [0]  tail losses 4.28816  head loss: 0.17470\n",
            " Epoch [0]  tail losses 4.28491  head loss: 0.17480\n",
            " Epoch [0]  tail losses 4.28166  head loss: 0.17490\n",
            " Epoch [0]  tail losses 4.27840  head loss: 0.17500\n",
            " Epoch [0]  tail losses 4.27515  head loss: 0.17510\n",
            " Epoch [0]  tail losses 4.27193  head loss: 0.17521\n",
            " Epoch [0]  tail losses 4.26869  head loss: 0.17531\n",
            " Epoch [0]  tail losses 4.26547  head loss: 0.17541\n",
            " Epoch [0]  tail losses 4.26226  head loss: 0.17551\n",
            " Epoch [0]  tail losses 4.25906  head loss: 0.17561\n",
            " Epoch [0]  tail losses 4.25585  head loss: 0.17570\n",
            " Epoch [0]  tail losses 4.25263  head loss: 0.17580\n",
            " Epoch [0]  tail losses 4.24945  head loss: 0.17590\n",
            " Epoch [0]  tail losses 4.24625  head loss: 0.17600\n",
            " Epoch [0]  tail losses 4.24308  head loss: 0.17610\n",
            " Epoch [0]  tail losses 4.23988  head loss: 0.17620\n",
            " Epoch [0]  tail losses 4.23670  head loss: 0.17629\n",
            " Epoch [0]  tail losses 4.23353  head loss: 0.17639\n",
            " Epoch [0]  tail losses 4.23035  head loss: 0.17649\n",
            " Epoch [0]  tail losses 4.22716  head loss: 0.17659\n",
            " Epoch [0]  tail losses 4.22399  head loss: 0.17669\n",
            " Epoch [0]  tail losses 4.22081  head loss: 0.17678\n",
            " Epoch [0]  tail losses 4.21762  head loss: 0.17688\n",
            " Epoch [0]  tail losses 4.21447  head loss: 0.17698\n",
            " Epoch [0]  tail losses 4.21128  head loss: 0.17707\n",
            " Epoch [0]  tail losses 4.20810  head loss: 0.17717\n",
            " Epoch [0]  tail losses 4.20497  head loss: 0.17727\n",
            " Epoch [0]  tail losses 4.20182  head loss: 0.17736\n",
            " Epoch [0]  tail losses 4.19867  head loss: 0.17746\n",
            " Epoch [0]  tail losses 4.19553  head loss: 0.17755\n",
            " Epoch [0]  tail losses 4.19240  head loss: 0.17765\n",
            " Epoch [0]  tail losses 4.18927  head loss: 0.17774\n",
            " Epoch [0]  tail losses 4.18616  head loss: 0.17783\n",
            " Epoch [0]  tail losses 4.18305  head loss: 0.17793\n",
            " Epoch [0]  tail losses 4.17994  head loss: 0.17802\n",
            " Epoch [0]  tail losses 4.17683  head loss: 0.17811\n",
            " Epoch [0]  tail losses 4.17372  head loss: 0.17821\n",
            " Epoch [0]  tail losses 4.17062  head loss: 0.17830\n",
            " Epoch [0]  tail losses 4.16752  head loss: 0.17839\n",
            " Epoch [0]  tail losses 4.16445  head loss: 0.17849\n",
            " Epoch [0]  tail losses 4.16135  head loss: 0.17858\n",
            " Epoch [0]  tail losses 4.15825  head loss: 0.17867\n",
            " Epoch [0]  tail losses 4.15517  head loss: 0.17876\n",
            " Epoch [0]  tail losses 4.15212  head loss: 0.17885\n",
            " Epoch [0]  tail losses 4.14906  head loss: 0.17894\n",
            " Epoch [0]  tail losses 4.14597  head loss: 0.17903\n",
            " Epoch [0]  tail losses 4.14292  head loss: 0.17912\n",
            " Epoch [0]  tail losses 4.13985  head loss: 0.17921\n",
            " Epoch [0]  tail losses 4.13680  head loss: 0.17930\n",
            " Epoch [0]  tail losses 4.13375  head loss: 0.17939\n",
            " Epoch [0]  tail losses 4.13071  head loss: 0.17948\n",
            " Epoch [0]  tail losses 4.12766  head loss: 0.17957\n",
            " Epoch [0]  tail losses 4.12463  head loss: 0.17966\n",
            " Epoch [0]  tail losses 4.12161  head loss: 0.17975\n",
            " Epoch [0]  tail losses 4.11855  head loss: 0.17984\n",
            " Epoch [0]  tail losses 4.11553  head loss: 0.17992\n",
            " Epoch [0]  tail losses 4.11248  head loss: 0.18001\n",
            " Epoch [0]  tail losses 4.10947  head loss: 0.18010\n",
            " Epoch [0]  tail losses 4.10646  head loss: 0.18019\n",
            " Epoch [0]  tail losses 4.10343  head loss: 0.18028\n",
            " Epoch [0]  tail losses 4.10041  head loss: 0.18036\n",
            " Epoch [0]  tail losses 4.09739  head loss: 0.18045\n",
            " Epoch [0]  tail losses 4.09437  head loss: 0.18054\n",
            " Epoch [0]  tail losses 4.09138  head loss: 0.18062\n",
            " Epoch [0]  tail losses 4.08838  head loss: 0.18071\n",
            " Epoch [0]  tail losses 4.08540  head loss: 0.18079\n",
            " Epoch [0]  tail losses 4.08241  head loss: 0.18088\n",
            " Epoch [0]  tail losses 4.07943  head loss: 0.18096\n",
            " Epoch [0]  tail losses 4.07646  head loss: 0.18105\n",
            " Epoch [0]  tail losses 4.07349  head loss: 0.18113\n",
            " Epoch [0]  tail losses 4.07050  head loss: 0.18122\n",
            " Epoch [0]  tail losses 4.06751  head loss: 0.18130\n",
            "Training complete\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "    Kitsune Evaluation\n",
        "\"\"\"\n",
        "\n",
        "# data = \"../Data/traffic.csv\"\n",
        "\n",
        "# Extract features\n",
        "# print(\"Extracting features\")\n",
        "# extract_features(\"../Data/traffic.tsv\", \"../Data/traffic.csv\")\n",
        "\n",
        "\n",
        "# Train model on \"../Data/traffic.csv\"\n",
        "# print(\"Training model on \" + data)\n",
        "# model = KitModel()\n",
        "\n",
        "# # Train model\n",
        "# model.train_model(data)\n",
        "# print(\"Training complete\")\n",
        "\n",
        "# # Store model\n",
        "# model.store_model(\"kitsune.pt\")\n",
        "\n",
        "\"\"\"\n",
        "    Keyed Kitsune Evaluation\n",
        "\"\"\"\n",
        "\n",
        "data = \"../Data/_transformed_traffic.csv\"\n",
        "\n",
        "# # Transform features\n",
        "# print(\"Transforming features\")\n",
        "# transform_features(\"../Data/traffic.csv\", \"../Data/transformed_traffic.csv\")\n",
        "\n",
        "# Train model on \"../Data/traffic.csv\"\n",
        "print(\"Training Keyed Kitsune model on \" + data)\n",
        "model = KeyedKitModel()\n",
        "\n",
        "# Train model\n",
        "model.train_model(data)\n",
        "print(\"Training complete\")\n",
        "\n",
        "# Store model\n",
        "model.store_model(\"KeyedKitsune.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gweftj1V-NXE",
        "outputId": "a7c70b40-55a3-4116-d262-bbce8b51d8c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluting ../Data/_transformed_traffic.csv\n",
            "meta None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "764111it [55:32, 258.46it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score saved to kitsune_score.csv\n",
            "kitsune prediction saved to kitsune_prediction.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/himanshi/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1029: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total packets: 764136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "764136it [55:41, 228.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plot path: KeyedKitsuneRes_png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(139, 0.24804265159689218)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# eval_kitsune(path,K,record_prediction=True)\n",
        "eval_kitsune(data, model, threshold=None, out_image=\"KeyedKitsuneRes_png\", meta_file=None, record_scores=True, y_true=None, record_prediction=True, load_prediction=False, plot_with_time=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qwDhrlu_Gbj",
        "outputId": "b620d6f5-de01-4554-cbaf-99b1814ddae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'adversarial-kitsune'...\n",
            "Password for 'https://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@github.com': "
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/swainsubrat/adversarial-kitsune.git\n",
        "!git clone https://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@github.com/swainsubrat/adversarial-kitsune.git\n",
        "\n",
        "# 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def evaluation_metrics(scores_path, threshold_path):\n",
        "    # Calulate the F1 score for Model using ground labels\n",
        "    # Actual outputs from kitsune_score.csv\n",
        "    y_output = csv.reader(open(scores_path, 'r'))\n",
        "    y_output = list(y_output)\n",
        "    y_output = [float(i[0]) for i in y_output]\n",
        "\n",
        "    # let ground labels be 0(benign) for all pkts\n",
        "    y_true = np.ones(len(y_output))\n",
        "\n",
        "    # Threshold for F1 score\n",
        "    threshold = csv.reader(open(threshold_path, 'r'))\n",
        "    threshold = list(threshold)\n",
        "    threshold = float(threshold[0][0])\n",
        "\n",
        "    # Predicted labels\n",
        "    y_pred = np.array([0 if i > threshold else 1 for i in y_output])\n",
        "\n",
        "    # 0 -> benign and 1 -> malicious\n",
        "\n",
        "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    f1 = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "    print(\"True Negatives: \", tn)\n",
        "    print(\"False Positives: \", fp)\n",
        "    print(\"False Negatives: \", fn)\n",
        "    print(\"True Positives: \", tp)\n",
        "\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1 score: \", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "UO97zWTVt3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation metrics for Kitsune: \n",
            "True Negatives:  0\n",
            "False Positives:  0\n",
            "False Negatives:  64\n",
            "True Positives:  764072\n",
            "Precision:  1.0\n",
            "Recall:  0.9999162452757101\n",
            "F1 score:  0.9999581208840681\n",
            "\n",
            "Evaluation metrics for Keyed Kitsune: \n",
            "True Negatives:  0\n",
            "False Positives:  0\n",
            "False Negatives:  139\n",
            "True Positives:  763997\n",
            "Precision:  1.0\n",
            "Recall:  0.9998180952081829\n",
            "F1 score:  0.9999090393310006\n"
          ]
        }
      ],
      "source": [
        "# Evaluation metrics for Kitsune\n",
        "scores_path = '../results/Kitsune/kitsune_score.csv'\n",
        "threshold_path = '../results/Kitsune/kitsune_threshold.csv'\n",
        "\n",
        "print(\"Evaluation metrics for Kitsune: \")\n",
        "evaluation_metrics(scores_path, threshold_path)\n",
        "\n",
        "print()\n",
        "\n",
        "# Evaluation metrics for Keyed Kitsune\n",
        "scores_path = '../results/KeyedKitsune/kitsune_score.csv'\n",
        "threshold_path = '../results/KeyedKitsune/kitsune_threshold.csv'\n",
        "\n",
        "print(\"Evaluation metrics for Keyed Kitsune: \")\n",
        "evaluation_metrics(scores_path, threshold_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
